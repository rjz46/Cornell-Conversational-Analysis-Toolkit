{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Corpus conversion",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjz46/Cornell-Conversational-Analysis-Toolkit/blob/katy/Corpus_conversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lw_Wr7Evp6dO"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U4lPXUFEECJv",
        "outputId": "56aa711f-ef33-47d3-fcdb-763e9be12501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install convokit\n",
        "!python3 -m spacy download en\n",
        "import nltk; nltk.download('punkt')\n",
        "import convokit"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting convokit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f3/d04f33525a2dbf316c31bfab383d1ee84280714b474718e272882e8b2d6a/convokit-2.0.11.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from convokit) (3.0.3)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.6/dist-packages (from convokit) (0.24.2)\n",
            "Collecting msgpack-numpy==0.4.3.2 (from convokit)\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/45/464be6da85b5ca893cfcbd5de3b31a6710f636ccb8521b17bd4110a08d94/msgpack_numpy-0.4.3.2-py2.py3-none-any.whl\n",
            "Collecting spacy==2.0.12 (from convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/de/ac14cd453c98656d6738a5669f96a4ac7f668493d5e6b78227ac933c5fd4/spacy-2.0.12.tar.gz (22.0MB)\n",
            "\u001b[K     |████████████████████████████████| 22.0MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from convokit) (1.3.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from convokit) (0.21.3)\n",
            "Collecting nltk>=3.4 (from convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 42.4MB/s \n",
            "\u001b[?25hCollecting dill==0.2.9 (from convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/42/bfe2e0857bc284cbe6a011d93f2a9ad58a22cb894461b199ae72cfef0f29/dill-0.2.9.tar.gz (150kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 46.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (2.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (1.16.5)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.4->convokit) (2018.9)\n",
            "Requirement already satisfied: msgpack>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from msgpack-numpy==0.4.3.2->convokit) (0.5.6)\n",
            "Collecting murmurhash<0.29,>=0.28 (from spacy==2.0.12->convokit)\n",
            "  Downloading https://files.pythonhosted.org/packages/82/55/7f050e9f73c9a58df219c63e77304b0ff01676847061dc99abb484cff3a8/murmurhash-0.28.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting cymem<1.32,>=1.30 (from spacy==2.0.12->convokit)\n",
            "  Downloading https://files.pythonhosted.org/packages/a5/0f/d29aa68c55db37844c77e7e96143bd96651fd0f4453c9f6ee043ac846b77/cymem-1.31.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting preshed<2.0.0,>=1.0.0 (from spacy==2.0.12->convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/88/57a818051f3d71e800bfb7ba4df56d3ea5793482ef11f1d2109b726f3bac/preshed-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 27.9MB/s \n",
            "\u001b[?25hCollecting thinc<6.11.0,>=6.10.3 (from spacy==2.0.12->convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/b1/47a88072d0a38b3594c0a638a62f9ef7c742b8b8a87f7b105f7ed720b14b/thinc-6.10.3.tar.gz (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.12->convokit) (0.9.6)\n",
            "Collecting ujson>=1.35 (from spacy==2.0.12->convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 44.1MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5 (from spacy==2.0.12->convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.12->convokit) (2.21.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->convokit) (0.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.4->convokit) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->convokit) (41.2.0)\n",
            "Collecting cytoolz<0.10,>=0.9.0 (from thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f4/9728ba01ccb2f55df9a5af029b48ba0aaca1081bbd7823ea2ee223ba7a42/cytoolz-0.9.0.1.tar.gz (443kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 35.5MB/s \n",
            "\u001b[?25hCollecting wrapt<1.11.0,>=1.10.0 (from thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit)\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit) (4.28.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit) (3.0.4)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit) (0.10.0)\n",
            "Building wheels for collected packages: convokit, spacy, nltk, dill, thinc, ujson, regex, cytoolz, wrapt\n",
            "  Building wheel for convokit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convokit: filename=convokit-2.0.11-cp36-none-any.whl size=82047 sha256=66da1248c2a5d3c51477aeac0c668a11df28014cd66e20f1f7f3082c3877b90d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/74/ba/90b7717e5dcfcc83ce63c34cc6e20d60ef850381dba5b3a0c9\n",
            "  Building wheel for spacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy: filename=spacy-2.0.12-cp36-cp36m-linux_x86_64.whl size=29062704 sha256=a35171b2b2fded2d6845a7adc9b9f3650b27b253436d7ea855c74f4d1e8b5520\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/0b/bb/7c2e28db574dbb2358176934eddd32a1c5f838ba0bc23eaaab\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449908 sha256=df51570bf1b5f3b65f1e9eff8fb86d633922c60aa234e0a3822a417a39a856a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.2.9-cp36-none-any.whl size=77403 sha256=0dd6825411e7299362f62269b4e186f941fe5ead0367463d0e3d71769855fe59\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/d7/0f/e58eae695403de585269f4e4a94e0cd6ca60ec0c202936fa4a\n",
            "  Building wheel for thinc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thinc: filename=thinc-6.10.3-cp36-cp36m-linux_x86_64.whl size=4120044 sha256=e3b320cac7bc796cfe4bcb7367b34103da2c48d803995eb5d78864d044d1588b\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/bc/e1/9b321b6b203288cf636a56e668ed5700076af4ed66062278ca\n",
            "  Building wheel for ujson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ujson: filename=ujson-1.35-cp36-cp36m-linux_x86_64.whl size=68036 sha256=625169d92c57ced1454f7d45589fe42293fd9e6fa3a05038b12c50441dd60725\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533190 sha256=a9037cb91aff6a61afe8223fb98a43d1d904c4d699a469e306bdcc5664506a58\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.9.0.1-cp36-cp36m-linux_x86_64.whl size=1247759 sha256=0ceec360a810628300e99c34c1033c2fc9fe65eb160c3702e897ce7cf168f2c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/f3/11/9817b001e59ab04889e8cffcbd9087e2e2155b9ebecfc8dd38\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.10.11-cp36-cp36m-linux_x86_64.whl size=65120 sha256=eb8fe68f1349a11caea782bf7477ea94954f302140c25bafadaa5a4354132fa9\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/5d/04/22361a593e70d23b1f7746d932802efe1f0e523376a74f321e\n",
            "Successfully built convokit spacy nltk dill thinc ujson regex cytoolz wrapt\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement wrapt>=1.11.1, but you'll have wrapt 1.10.11 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: multiprocess 0.70.8 has requirement dill>=0.3.0, but you'll have dill 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.57 has requirement spacy>=2.0.18, but you'll have spacy 2.0.12 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.1.0 has requirement spacy>=2.1.0, but you'll have spacy 2.0.12 which is incompatible.\u001b[0m\n",
            "Installing collected packages: msgpack-numpy, murmurhash, cymem, preshed, cytoolz, wrapt, dill, thinc, ujson, regex, spacy, nltk, convokit\n",
            "  Found existing installation: murmurhash 1.0.2\n",
            "    Uninstalling murmurhash-1.0.2:\n",
            "      Successfully uninstalled murmurhash-1.0.2\n",
            "  Found existing installation: cymem 2.0.2\n",
            "    Uninstalling cymem-2.0.2:\n",
            "      Successfully uninstalled cymem-2.0.2\n",
            "  Found existing installation: preshed 2.0.1\n",
            "    Uninstalling preshed-2.0.1:\n",
            "      Successfully uninstalled preshed-2.0.1\n",
            "  Found existing installation: wrapt 1.11.2\n",
            "    Uninstalling wrapt-1.11.2:\n",
            "      Successfully uninstalled wrapt-1.11.2\n",
            "  Found existing installation: dill 0.3.0\n",
            "    Uninstalling dill-0.3.0:\n",
            "      Successfully uninstalled dill-0.3.0\n",
            "  Found existing installation: thinc 7.0.8\n",
            "    Uninstalling thinc-7.0.8:\n",
            "      Successfully uninstalled thinc-7.0.8\n",
            "  Found existing installation: spacy 2.1.8\n",
            "    Uninstalling spacy-2.1.8:\n",
            "      Successfully uninstalled spacy-2.1.8\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed convokit-2.0.11 cymem-1.31.2 cytoolz-0.9.0.1 dill-0.2.9 msgpack-numpy-0.4.3.2 murmurhash-0.28.0 nltk-3.4.5 preshed-1.0.1 regex-2017.4.5 spacy-2.0.12 thinc-6.10.3 ujson-1.35 wrapt-1.10.11\n",
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K     |████████████████████████████████| 37.4MB 18.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.0.0-cp36-none-any.whl size=37405977 sha256=bddf41cfba99cedefd9cb03e2eb6be85b2f1783752e151bc91a9a9b6bb0568b9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0ojhjx5_/wheels/54/7c/d8/f86364af8fbba7258e14adae115f18dd2c91552406edc3fdaa\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.1.0\n",
            "    Uninstalling en-core-web-sm-2.1.0:\n",
            "      Successfully uninstalled en-core-web-sm-2.1.0\n",
            "Successfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gv0S3-_bEybB",
        "outputId": "dff1fe6e-4e35-43d9-987c-a32c03899846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import google\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VHJBebM8qF0m",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from IPython.display import display as disp\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "74FCvanlQ2d0"
      },
      "source": [
        "#Users: initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "zRcql6DNp08l",
        "outputId": "cbf87299-5c2d-4111-bf49-c5860b017fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#@title Load json and make dict called \"all_users\" \n",
        "user_names = set()\n",
        "DELETED_USER_NAME = '[deleted]'\n",
        "\n",
        "with open('gdrive/My Drive/CS6742/coarse_discourse_dump_reddit_with_votes.json', 'r') as f:\n",
        "  for line in f:\n",
        "    json_obj = json.loads(line)\n",
        "    for post in json_obj['posts']:\n",
        "      if 'author' in post:\n",
        "        user_names.add(post['author'])\n",
        "      else:\n",
        "        # I spot checked a post and comment, and it seems that a missing 'author' means it was deleted (post: t3_29mjj7, comment: t1_crzi5yt in reply to t3_391dim)\n",
        "        user_names.add(DELETED_USER_NAME)\n",
        "\n",
        "all_users = {user_name: convokit.User(name=user_name) for user_name in user_names}\n",
        "\n",
        "print(\"Created {} users\".format(len(all_users)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created 63573 users\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gc4D92xkx2UG",
        "outputId": "b6e36ed8-0b96-41f1-aa53-ede2671ca3e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "source": [
        "#@title Check results\n",
        "print(\"\\nSample users:\")\n",
        "disp(list(all_users.items())[:3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sample users:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('reborntv', User([('name', 'reborntv')])),\n",
              " ('Beardacus5', User([('name', 'Beardacus5')])),\n",
              " ('WikipediaPoster', User([('name', 'WikipediaPoster')]))]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s6rBMo6JQ6gs"
      },
      "source": [
        "#Utterances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "6HllJBAYQ7wi",
        "colab": {}
      },
      "source": [
        "#@title Make \"utterance_corpus\" dict\n",
        "'''\n",
        "\n",
        "id: index of the utterance\n",
        "user: the user who author the utterance\n",
        "root: index of the conversation root of the utterance\n",
        "reply_to: index of the utterance to which this utterance replies to (None if the utterance is not a reply)\n",
        "timestamp: time of the utterance\n",
        "text: textual content of the utterance\n",
        "\n",
        "meta: dictionary of utterance metadata\n",
        "    post_depth, \n",
        "    majority type\n",
        "    ann_types\n",
        "    majority_linkann_links\n",
        "    ups\n",
        "    \n",
        "'''\n",
        "user_names = set()\n",
        "DELETED_USER_NAME = '[deleted]'\n",
        "\n",
        "with open('gdrive/My Drive/CS6742/coarse_discourse_dump_reddit_with_votes.json', 'r') as f:\n",
        "  for line in f:\n",
        "    json_obj = json.loads(line)\n",
        "    for post in json_obj['posts']:\n",
        "      if 'author' in post:\n",
        "        user_names.add(post['author'])\n",
        "      else:\n",
        "        # I spot checked a post and comment, and it seems that a missing 'author' means it was deleted (post: t3_29mjj7, comment: t1_crzi5yt in reply to t3_391dim)\n",
        "        user_names.add(DELETED_USER_NAME)\n",
        "\n",
        "all_users = {user_name: convokit.User(name=user_name) for user_name in user_names}\n",
        "\n",
        "utterance_corpus = {}\n",
        "      \n",
        "with open('gdrive/My Drive/CS6742/coarse_discourse_dump_reddit_with_votes.json', 'r') as f:\n",
        "    for line in f:\n",
        "      json_obj = json.loads(line)\n",
        "\n",
        "      root = json_obj['posts'][0]['id']\n",
        "\n",
        "      for post in json_obj['posts']:\n",
        "        \n",
        "        idx = post['id'] \n",
        "        text = post['body'] if 'body' in post else \"\"\n",
        "        user, post_depth, in_reply_to = None, None, None\n",
        "        \n",
        "        if 'author' in post:\n",
        "          user = post['author']\n",
        "        else:\n",
        "          user = DELETED_USER_NAME\n",
        "\n",
        "        if 'is_first_post' in post:\n",
        "          in_reply_to = None\n",
        "          post_depth = 0\n",
        "        else:\n",
        "          in_reply_to = post['in_reply_to']\n",
        "          post_depth = post['post_depth']\n",
        "\n",
        "        annotations = post['annotations']  \n",
        "  \n",
        "        metadata = {\n",
        "          'post_depth': post_depth,\n",
        "          'majority_type': post['majority_type'] if 'majority_type' in post else None,\n",
        "          'majority_link': post['majority_link'] if 'majority_link' in post else None,\n",
        "          'annotation-types': [annotation['main_type'] for annotation in annotations],\n",
        "          'annotation-links': [annotation['link_to_post'] if 'link_to_post' in annotation else None for annotation in annotations],\n",
        "          'ups': post['ups'] if 'ups' in post else None\n",
        "\n",
        "        }\n",
        "\n",
        "        utterance_corpus[idx] = convokit.Utterance(idx, all_users[user], root, in_reply_to, None, text, meta=metadata)\n",
        "      \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5AzgTHJ19N0O",
        "outputId": "f5cce9eb-3dde-40a9-a951-8ab913d1f751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "print(len(user_names))\n",
        "print(len(utterance_corpus))\n",
        "disp(list(utterance_corpus.items())[:10])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63573\n",
            "115827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('t3_1bx6qw',\n",
              "  Utterance({'id': 't3_1bx6qw', 'user': User([('name', 'DTX120')]), 'root': 't3_1bx6qw', 'reply_to': None, 'timestamp': None, 'text': \"4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/title/tt0073440/reference\\n\\nIt was only a few minutes into Robert Altman's homespun epic *Nashville* that I got the feeling I was watching a great movie. By the end it could not be denied. Now I'm sure it helps that I'm a musician, since this created an immediate connection to the subject matter. I spent a portion of the movie with my Telecaster in my lap trying to play along with the characters who all seem to be really playing and singing these songs. However I also am *not* a fan of country western, so that could have easily been a turn off.  \\n\\nTo begin describing the action in the film is daunting. I can't even process a lot of what I saw. This movie is extremely dense, and the first 30 minutes or so are spent just trying to figure out who people are. Their relationships to one another - some of which are purely incidental - slowly become clear as things progress. It's an ensemble cast with no clear lead and lots of overlapping conversations. Some developments all tie together in the end. Some seem to be kind of loose ends. There is throughout, however, a very keen sense about people. These naturalistic performances are believable and so is the world they inhabit. It feels like a film that doesn't exaggerate - rather, it downplays some of the outrageousness which just makes it seem all the more outrageous. Consider for example Shelley Duvall's wig or Jeff Goldblum's entire character.  \\n\\nI feel I should mention this. Jeff Goldblum is in this film. He never speaks, and he is awesome.  \\n\\n*Nashville* is a musical. There is reportedly around an hour of music in its 150+ minute run time. I believe it. Sometimes these songs, most if not all of which are original, seem to speak pointedly to or about a character. Sometimes it's all about the shifts and glances in the audience. There are many memorable scenes, such as when a fragile country starlet spaces out on stage and begins rambling between numbers, or a disenchanted housewife waits at the back of a bar for a womanizing folk singer, or a poor deceived waitress is goaded into a striptease by a raucous crowd and still refuses to believe that she can't actually sing.   \\n\\nThis is rich film, a complex tapestry perfectly suited to a moment in time (the US Bicentennial) that seems as though it will reward additional viewings. I sometimes become fidgety during long movies but I was never bored with *Nashville*. It swept me along with its quirky and interesting characters to a climax that maybe I should have seen coming.  \\n\\n8/10\", 'meta': {'post_depth': 0, 'majority_type': 'announcement', 'majority_link': 'none', 'annotation-types': ['announcement', 'announcement', 'announcement'], 'annotation-links': ['none', 'none', 'none'], 'ups': 3}})),\n",
              " ('t1_c9b2nyd',\n",
              "  Utterance({'id': 't1_c9b2nyd', 'user': User([('name', 'mcgrewf10')]), 'root': 't3_1bx6qw', 'reply_to': 't3_1bx6qw', 'timestamp': None, 'text': \"I've wanted to watch this for a long time. I was also turned off by the country western aspect.\", 'meta': {'post_depth': 1, 'majority_type': 'elaboration', 'majority_link': 't3_1bx6qw', 'annotation-types': ['agreement', 'elaboration', 'elaboration'], 'annotation-links': ['t3_1bx6qw', 't3_1bx6qw', 't3_1bx6qw'], 'ups': 2}})),\n",
              " ('t1_c9b30i1',\n",
              "  Utterance({'id': 't1_c9b30i1', 'user': User([('name', 'DTX120')]), 'root': 't3_1bx6qw', 'reply_to': 't1_c9b2nyd', 'timestamp': None, 'text': \"You strike me as the type who would appreciate it. I would give it a go. This is also my first Altman film so I didn't really know what to expect, except that people always compare PTA's Boogie Nights and Magnolia as being influenced by Altman. Magnolia is probably the best analog in terms of structure (having no lead character) but it is stylistically very different, much more melodramatic and transparently earnest. \", 'meta': {'post_depth': 2, 'majority_type': 'elaboration', 'majority_link': 't1_c9b2nyd', 'annotation-types': ['elaboration', 'elaboration', 'elaboration'], 'annotation-links': ['t1_c9b2nyd', 't1_c9b2nyd', 't1_c9b2nyd'], 'ups': 1}})),\n",
              " ('t1_c9b6sj0',\n",
              "  Utterance({'id': 't1_c9b6sj0', 'user': User([('name', 'mcgrewf10')]), 'root': 't3_1bx6qw', 'reply_to': 't1_c9b30i1', 'timestamp': None, 'text': \"Yeah, I've always heard that Altman was famous for his ensemble casts. But I, too, have never seen an Altman.\", 'meta': {'post_depth': 3, 'majority_type': 'elaboration', 'majority_link': 't1_c9b30i1', 'annotation-types': ['agreement', 'elaboration', 'elaboration'], 'annotation-links': ['t1_c9b30i1', 't1_c9b30i1', 't1_c9b30i1'], 'ups': 1}})),\n",
              " ('t3_omv7p',\n",
              "  Utterance({'id': 't3_omv7p', 'user': User([('name', 'Keatonus')]), 'root': 't3_omv7p', 'reply_to': None, 'timestamp': None, 'text': \"Alright guys, little background about myself. I'm a good looking, 23 year old male, have had modest success in the past with women, but have decided that modest just isn't good enough anymore.\\n\\nThe problem has never been a lack of attention, or opportunity. It's just been not having the killer instinct and extreme AA, to the point where women will basically EYE FUCK me, and I still don't have the testicular fortitude to say anything unless they say something first.\\n\\nMy goal is to approach 100 sets by the end of April, and hopefully break my AA.\", 'meta': {'post_depth': 0, 'majority_type': 'announcement', 'majority_link': 'none', 'annotation-types': ['announcement', 'announcement', 'announcement'], 'annotation-links': ['none', 'none', 'none'], 'ups': 6}})),\n",
              " ('t1_c3igqif',\n",
              "  Utterance({'id': 't1_c3igqif', 'user': User([('name', 'Keatonus')]), 'root': 't3_omv7p', 'reply_to': 't3_omv7p', 'timestamp': None, 'text': '**January 16th 3 Sets:** \\nWent out shopping with my grandma as I visited her. We go to a Factory store. Opened 3 sets.\\n\\n* First Set: was the changing room gal, HB 6 cute asian gal. Asked her a lot of questions about what i was wearing. Made her laugh several times with corny jokes, introduced her to my grandma, and we all talked for a minute after I was done picking out the clothes I wanted and left. \\n\\n* Second set: was this older gal, mid 40\\'s. Wouldn\\'t give her an HB because it was more of just a casual chat about the outfit I put on. She said she didn\\'t like it, I responded with \"aw, I\\'m not attractive?!\" and she said \"I never said that haha\" and grinned. I gave her a wink as she continued laughing and went into her stall.\\n\\n* Third set:  **NUMBER CLOSE** As me and my Grandma are waiting in line to be helped checking out, and HB7 walks to the register and helps us. We IMMEDIATELY small talk. Exchanging intellectual talking points such as martyrs and assassinations, making her laugh with jokes, poking fun at her, transitioned to talking about movies, and she suggested I watch one. The Darjeeling Limited, I gave her a queer look, she then rolled out some receipt to write it down. I asked her to put her name down there too. She did, and then said \"Want my number so you can tell me when you watch it?\" I obliged her saying \"Yeah, that could work\". It was only after I got her number she even ACKNOWLEDGED my grandmother being ONE FOOT next to me the whole time! HAHA. She asked who she was, I said my Grandmother. Mind you I look completely white, and my grandmother is full Japanese. We talked for a bit about my grandma and the war, and how my grandma was bombed etc. Then I take my leave and text her about an hour later. We have been talking since.\\n\\nTL;DR: Got a number with my Grandma right next to me!', 'meta': {'post_depth': 1, 'majority_type': 'elaboration', 'majority_link': 't3_omv7p', 'annotation-types': ['elaboration', 'elaboration', 'elaboration'], 'annotation-links': ['t3_omv7p', 't3_omv7p', 't3_omv7p'], 'ups': 5}})),\n",
              " ('t1_c3imrkb',\n",
              "  Utterance({'id': 't1_c3imrkb', 'user': User([('name', 'rakendrachen')]), 'root': 't3_omv7p', 'reply_to': 't1_c3igqif', 'timestamp': None, 'text': 'grandmas are the best wingmen.', 'meta': {'post_depth': 2, 'majority_type': 'humor', 'majority_link': 't1_c3igqif', 'annotation-types': ['humor', 'appreciation', 'humor'], 'annotation-links': ['t1_c3igqif', 't1_c3igqif', 't1_c3igqif'], 'ups': 3}})),\n",
              " ('t1_c3ij8z0',\n",
              "  Utterance({'id': 't1_c3ij8z0', 'user': User([('name', 'answerorreply')]), 'root': 't3_omv7p', 'reply_to': 't3_omv7p', 'timestamp': None, 'text': \"dude, these sets are awesome. You're doing great. Sounds like you're a natural at meeting people once you get past the AA.\", 'meta': {'post_depth': 1, 'majority_type': 'appreciation', 'majority_link': 't3_omv7p', 'annotation-types': ['appreciation', 'appreciation', 'appreciation'], 'annotation-links': ['t3_omv7p', 't3_omv7p', 't3_omv7p'], 'ups': 2}})),\n",
              " ('t1_c3inx9t',\n",
              "  Utterance({'id': 't1_c3inx9t', 'user': User([('name', 'Keatonus')]), 'root': 't3_omv7p', 'reply_to': 't1_c3ij8z0', 'timestamp': None, 'text': 'Thanks man! Yeah I\\'m trying to just keep the \"who cares have fun\" attitude on. Because normally I freak out about what to say. But I find if I just bring a subject up and get some momentum going, I\\'m actually a pretty good conversationalist.', 'meta': {'post_depth': 2, 'majority_type': 'appreciation', 'majority_link': 't1_c3ij8z0', 'annotation-types': ['appreciation', 'appreciation', 'appreciation'], 'annotation-links': ['t1_c3ij8z0', 't1_c3ij8z0', 't1_c3ij8z0'], 'ups': 1}})),\n",
              " ('t1_c3k2lc1',\n",
              "  Utterance({'id': 't1_c3k2lc1', 'user': User([('name', 'Keatonus')]), 'root': 't3_omv7p', 'reply_to': 't3_omv7p', 'timestamp': None, 'text': \"Ok. Update! Sorry I haven't been doing this day to day like I should be. Let me see what I can remember\\n\\n**January 17-23 Sets:** Various sets I remember opening, unfortunately no number closes. Just good old fashioned chit chatting.\\n\\n* 4th set: During the 49ers and Giants game I went to grab a bite to eat at the local burger joint. Before ordering since no line was behind me I decided to small talk with the Female at the register HB5. Mostly talked about sports, a little about myself, and her coworker HB6 joined in the conversation. This lasted about 5 minutes, learned both had boyfriends through small talk and proceeded to order.\\n\\n* 5th set: Also small talked with the female who works at my local grocery chain. She was new there, we talked about the rain of all things haha. Male coworker semi interrupted us, still small talked with both. Then ordered my drink.\\n\\nBlahhh, I know there's more than this...Hence why I should try to update every day. Be back soon guys.\", 'meta': {'post_depth': 1, 'majority_type': 'elaboration', 'majority_link': 't3_omv7p', 'annotation-types': ['elaboration', 'elaboration', 'elaboration'], 'annotation-links': ['t3_omv7p', 't3_omv7p', 't3_omv7p'], 'ups': 1}}))]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aWyST0CERBu4"
      },
      "source": [
        "#Conversation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "EnGlyyKJREmd",
        "colab": {}
      },
      "source": [
        "#@title Make \"conversation_corpus\" dict\n",
        "\n",
        "conversation_corpus = {}\n",
        "with open('gdrive/My Drive/CS6742/coarse_discourse_dump_reddit_with_votes.json', 'r') as f:\n",
        "  for line in f:\n",
        "    json_obj = json.loads(line)\n",
        "    \n",
        "    posts = json_obj[\"posts\"]\n",
        "    first = posts[0]\n",
        "    \n",
        "    #print(first)\n",
        "    \n",
        "    if(first.get('author')!=None):\n",
        "      owner = first['author']\n",
        "    else:\n",
        "      owner = '[deleted]'\n",
        "    \n",
        "    convo_id = first['id'][:]\n",
        "    \n",
        "    comments = []\n",
        "    \n",
        "    for comment in posts[:]:\n",
        "      comments.append(comment['id'][:])\n",
        "    \n",
        "    meta_data = {\n",
        "        \"subreddit\": json_obj['subreddit'], \n",
        "        \"url\": json_obj['url'],\n",
        "        \"title\": json_obj['title']}\n",
        "    \n",
        "    conversation_corpus[convo_id] = convokit.Conversation(owner, convo_id, comments, meta_data)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hSkmNrNeti-f",
        "outputId": "7132cc2c-88fc-43ba-9b28-615265fbba47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(conversation_corpus['t3_1bx6qw'])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conversation({'_owner': 'DTX120', '_id': 't3_1bx6qw', '_utterance_ids': ['t3_1bx6qw', 't1_c9b2nyd', 't1_c9b30i1', 't1_c9b6sj0'], '_usernames': None, '_meta': {'subreddit': '100movies365days', 'url': 'https://www.reddit.com/r/100movies365days/comments/1bx6qw/dtx120_87_nashville/', 'title': 'DTX120: #87 - Nashville'}})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JtIeE635wzTH"
      },
      "source": [
        "# Users: add utterances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "-LFLT3tLxNC-",
        "outputId": "0f0d2212-b1f3-4372-bdb0-e591b59934ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "#@title Test that adding utterances to users also updates user object attached to utterances\n",
        "\n",
        "test_users = {user_name: convokit.User(name=user_name) for user_name in ['user1', 'user2', 'user3']}\n",
        "\n",
        "test_utterances = {\n",
        "    'id1.1': convokit.Utterance('id1.1', test_users['user1'], None, None, None, 'some_text'),\n",
        "    'id1.2': convokit.Utterance('id1.2', test_users['user1'], None, None, None, 'some_text'),\n",
        "    'id2.1': convokit.Utterance('id2.1', test_users['user2'], None, None, None, 'some_text'),\n",
        "}\n",
        "\n",
        "test_convos = {\n",
        "    'id1.1': convokit.Conversation(owner=None, id=\"id1.1\", utterances=['id1.1', 'id2.1']),\n",
        "    'id1.2': convokit.Conversation(owner=None, id=\"id1.2\", utterances=['id1.2'])\n",
        "    \n",
        "}\n",
        "\n",
        "\n",
        "for utt in test_utterances.values():\n",
        "  utt.user.utterances[utt.id] = utt\n",
        "  \n",
        "for i, convo in enumerate(test_convos.values()):\n",
        "  top_post = test_utterances[convo.id]\n",
        "  top_post.user.conversations[convo.id] = convo\n",
        "\n",
        "print(\"All test users, with associated utterance keys:\")\n",
        "for uid, user in test_users.items():\n",
        "  print(\"\\tUser ID: {} \\tUtterance keys: {}\".format(uid, list(user.utterances.keys())))\n",
        "  \n",
        "print()\n",
        "\n",
        "print(\"All test users, with associated convo keys:\")\n",
        "for uid, user in test_users.items():\n",
        "  print(\"\\tUser ID: {} \\tConvo keys: {}\".format(uid, list(user.conversations.keys())))\n",
        "  \n",
        "print()\n",
        "\n",
        "print(\"All test utterances, with associated user, and user's associated utterance keys:\")\n",
        "for utt_id, utt in test_utterances.items():\n",
        "  print(\"\\tUtt. ID: {} \\tUser ID: {} \\tUser's utt. keys: {}\".format(utt_id, utt.user.name, list(utt.user.utterances.keys())))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All test users, with associated utterance keys:\n",
            "\tUser ID: user1 \tUtterance keys: ['id1.1', 'id1.2']\n",
            "\tUser ID: user2 \tUtterance keys: ['id2.1']\n",
            "\tUser ID: user3 \tUtterance keys: []\n",
            "\n",
            "All test users, with associated convo keys:\n",
            "\tUser ID: user1 \tConvo keys: ['id1.1', 'id1.2']\n",
            "\tUser ID: user2 \tConvo keys: []\n",
            "\tUser ID: user3 \tConvo keys: []\n",
            "\n",
            "All test utterances, with associated user, and user's associated utterance keys:\n",
            "\tUtt. ID: id1.1 \tUser ID: user1 \tUser's utt. keys: ['id1.1', 'id1.2']\n",
            "\tUtt. ID: id1.2 \tUser ID: user1 \tUser's utt. keys: ['id1.1', 'id1.2']\n",
            "\tUtt. ID: id2.1 \tUser ID: user2 \tUser's utt. keys: ['id2.1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "B68lLhOL7lZe",
        "outputId": "2e94aa82-1209-4a4f-9a40-0ed06dfecfac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#@title Add utterances to all_users\n",
        "\n",
        "for i, utt in enumerate(utterance_corpus.values()):\n",
        "  utt.user.utterances[utt.id] = utt\n",
        "  \n",
        "print(\"Added {} utterances to all_users\".format(i+1))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Added 115827 utterances to all_users\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "l_s8hr3s8L5C",
        "outputId": "8fdf7cad-3f76-48e7-edf6-2a115b05f910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#@title Add convos to all_users\n",
        "for i, convo in enumerate(conversation_corpus.values()):\n",
        "  top_post = utterance_corpus[convo.id]\n",
        "  top_post.user.conversations[convo.id] = convo\n",
        "  \n",
        "print(\"Added {} convos to all_users\".format(i+1))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Added 9483 convos to all_users\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uZjT-W8hRFBq"
      },
      "source": [
        "#Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mOuhP1JPRG8t",
        "outputId": "b34d8d4c-67c3-42a5-9869-b46bfcb3bf23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "#@title Create corpus and add conversation metadata\n",
        "utterance_list = [utterance for k,utterance in utterance_corpus.items()]\n",
        "reddit_corpus = convokit.Corpus(utterances=utterance_list, version=1)\n",
        "\n",
        "for convo_id in reddit_corpus.get_conversation_ids():\n",
        "  reddit_corpus.get_conversation(convo_id).meta = conversation_corpus[convo_id].meta\n",
        "\n",
        "convo_ids = reddit_corpus.get_conversation_ids()\n",
        "for i, convo_idx in enumerate(convo_ids[0:5]):\n",
        "    print(\"sample conversation {}:\".format(convo_idx))\n",
        "    convo = reddit_corpus.get_conversation(convo_idx)\n",
        "    user_id = convo.get_utterance(convo.id).user.name\n",
        "    print(\"Meta: \", convo.meta)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample conversation t3_1bx6qw:\n",
            "Meta:  {'subreddit': '100movies365days', 'url': 'https://www.reddit.com/r/100movies365days/comments/1bx6qw/dtx120_87_nashville/', 'title': 'DTX120: #87 - Nashville'}\n",
            "sample conversation t3_omv7p:\n",
            "Meta:  {'subreddit': '100sets', 'url': 'https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/', 'title': 'Male, 23 years old. Going for 100 sets!'}\n",
            "sample conversation t3_259tbh:\n",
            "Meta:  {'subreddit': '1200isplenty', 'url': 'https://www.reddit.com/r/1200isplenty/comments/259tbh/122cal_black_currant_cheesecake/', 'title': '122cal black currant cheesecake!'}\n",
            "sample conversation t3_16h61h:\n",
            "Meta:  {'subreddit': '1911', 'url': 'https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/', 'title': 'Need help finding a Springfield!'}\n",
            "sample conversation t3_35igzp:\n",
            "Meta:  {'subreddit': '1911', 'url': 'https://www.reddit.com/r/1911/comments/35igzp/help_with_a_possible_trade/', 'title': 'Help with a possible trade?'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "0E0LEGX1QsBN",
        "colab": {}
      },
      "source": [
        "# @title Save to file and zip\n",
        "reddit_corpus.dump(\"reddit_coarse_discourse\", base_path = \"gdrive/My Drive/CS6742/\")\n",
        "\n",
        "with ZipFile(\"gdrive/My Drive/CS6742/reddit_coarse_discourse.zip\", 'w') as zip_f:\n",
        "  for fname in os.listdir(\"gdrive/My Drive/CS6742/reddit_coarse_discourse\"):\n",
        "    zip_f.write(\"gdrive/My Drive/CS6742/reddit_coarse_discourse/\"+fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "rirZ-FNFfVal",
        "outputId": "e65e4360-af13-42e2-8546-7ad67cd879b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "#@title Show stats\n",
        "\n",
        "print(len(reddit_corpus.get_utterance_ids()))\n",
        "print(len(reddit_corpus.get_conversation_ids()))\n",
        "print(len(reddit_corpus.get_usernames()))\n",
        "reddit_corpus.print_summary_stats()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "115827\n",
            "9483\n",
            "63573\n",
            "Number of Users: 63573\n",
            "Number of Utterances: 115827\n",
            "Number of Conversations: 9483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3_-wTJ-ftnN",
        "colab_type": "text"
      },
      "source": [
        "# Part D: Stats and politeness transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibl81EgsftMx",
        "colab_type": "code",
        "outputId": "0a37f240-34c0-4e16-fcaa-af3f5f0ab929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "reddit_corpus.print_summary_stats()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Users: 63573\n",
            "Number of Utterances: 115827\n",
            "Number of Conversations: 9483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnEDq9pNwa-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Prep corpus by running parser\n",
        "parser = convokit.Parser()\n",
        "temp_var = parser.transform(reddit_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeqRN2eI6-53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Save again\n",
        "temp_var.dump(\"reddit_coarse_discourse_with_parsed\", base_path = \"gdrive/My Drive/CS6742/\")\n",
        "\n",
        "with ZipFile(\"gdrive/My Drive/CS6742/reddit_coarse_discourse_with_parsed.zip\", 'w') as zip_f:\n",
        "  for fname in os.listdir(\"gdrive/My Drive/CS6742/reddit_coarse_discourse_with_parsed\"):\n",
        "    zip_f.write(\"gdrive/My Drive/CS6742/reddit_coarse_discourse_with_parsed/\"+fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llfl-txiAOPj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "8c584725-1dc2-42d4-bb8f-4b23781ca217"
      },
      "source": [
        "temp_var"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<convokit.model.corpus.Corpus at 0x7f7fa67563c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBAJRxZNzqWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Get politeness strategies\n",
        "\n",
        "ps = convokit.PolitenessStrategies()\n",
        "reddit_corpus = ps.transform(reddit_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}